# -*- coding: utf-8 -*-
"""Analytics Evaluation Question 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11AMgF7EWK_EC_0rQUAMT644fLAYVm_l_

A customer informed their consultant that they have developed several formulations of petrol 
that gives different characteristics of burning pattern. The formulations are obtaining by adding 
varying levels of additives that, for example, prevent engine knocking, gum prevention, stability 
in storage, and etc. However, a third party certification organisation would like to verify if the 
formulations are significantly different, and request for both physical and statistical proof. Since 
the formulations are confidential information, they are not named in the dataset.
"""

import pandas as pd
import numpy as np

data=pd.read_csv('/content/ingredient.csv')
data.head()

"""a. A descriptive analysis of the additives (columns named as “a” to “i”), which must include 
summaries of findings (parametric/non-parametric). Correlation and ANOVA, if applicable, is 
a must.
"""

data.describe()

"""From the std (standart deviation) we can see that column **a** and **i** have a low variance of data"""

data.shape

corData=data.corr().round(3)
corData

import seaborn as sn
import matplotlib.pyplot as plt

sn.heatmap(corData, annot=True)
plt.show()

"""from the heatmap we can conclude that columns A and G has a strong correlation """

from scipy.stats import skew
from scipy.stats import kurtosis
import math

"""Don't know about the ANOVA

b. A graphical analysis of the additives, including a distribution study
"""

yolo=data[['a']].value_counts().get(0)
if(pd.isna(yolo)):
  yolo=0

for i in data.columns:
  print(i);
  ax = sn.distplot(data[[i]],
                  bins=50,
                  kde=True,
                  color='blue',
                  hist_kws={"linewidth": 15,'alpha':1})
  ax.set(xlabel='Normal Distribution', ylabel='Frequency')
  plt.show()

for i in data.columns:
  count=data[[i]].value_counts().get(0);
  if(pd.isna(count)):
    count=0;
  print(i)
  print(count)
  print(skew(data[[i]],axis=0,bias=True));
  print(kurtosis(data[[i]],axis=0,bias=True));

"""From the skewness, kurotsis and the normal distrubution graph we can conclude that 


Attributes a,d,f,g,h,i are right-skewed\
Attributes c, h, f and i are zero-inflated\
Attribute c is bi-modal\
Attribute e is left-skewed.\
Attributes a,f,g,h are Leptokurtic (it signifies that it tries to produce more outliers rather than the normal distribution.\
Attributes b,e are mesokurtic that have normal distribution\
Attributes c,d,i are platykurtics that have thinner tails than a normal distribution

c. A clustering test of your choice (unsupervised learning), to determine the distinctive number 
of formulations present in the dataset.
"""

from sklearn.cluster import KMeans

X = data.values
X = np.nan_to_num(X)

# Collecting the distortions into list
distortions = []
K = range(1,10)
for k in K:
 kmeanModel = KMeans(n_clusters=k)
 kmeanModel.fit(X)
 distortions.append(kmeanModel.inertia_)
# Plotting the distortions
plt.figure(figsize=(16,8))
plt.plot(K, distortions, 'bx-')
plt.xlabel('k')
plt.ylabel('Distortion')
plt.title('The Elbow Method showing the optimal clusters')
plt.show()

distortions

pip install kneed

from kneed import KneeLocator
kl = KneeLocator(range(1, 10), distortions, curve="convex", direction="decreasing")
kl.elbow

"""From the elbow method we get the best number of cluster is 3"""

k_means =KMeans(init = "k-means++", n_clusters = kl.elbow)
predict=k_means.fit_predict(X)
predict

data['cluster']= predict
data

# Visualising the clusters
plt.scatter(X[predict== 0, 0], X[predict == 0, 1], s = 100, c = 'red')
plt.scatter(X[predict == 1, 0], X[predict == 1, 1], s = 100, c = 'blue')
plt.scatter(X[predict == 2, 0], X[predict == 2, 1], s = 100, c = 'green')
# Plotting the centroids of the clusters
plt.scatter(k_means.cluster_centers_[:, 0], k_means.cluster_centers_[:,1], s = 100, c = 'yellow', label = 'Centroids')
plt.legend()

